{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca02e66d-2987-468d-8a09-be3c459cc867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation Long Shhort-Term Memory with PyTorch + Lightning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.optim import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c89b392e-06c5-40fb-b451-9142887c3a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8adaccf-e326-4e7d-8f65-7b0b8ddf0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as l\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f83709e9-b254-47b7-815c-37ccf7ae3378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTMByHand inheritance l.LightningModule of Pytorch Lightning\n",
    "class LSTMByHand(l.LightningModule):\n",
    "    \n",
    "    # Create and initialize all Weight and Bias tensors that we need to implement an LSTM unit\n",
    "    def __init__(self):\n",
    "        # Call the initialization method for the parent class (LightningModule)\n",
    "        super().__init__()\n",
    "        # Use Normal Distribution to randomly select an initialization value for each Weight but Bias = 0\n",
    "        mean = torch.tensor(0.0)\n",
    "        std = torch.tensor(1.0)\n",
    "        \n",
    "        self.wlr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wlr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.blr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.wpr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wpr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bpr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bp1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bo1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "    # Do LSTM math\n",
    "    def lstm_unit(self, input_value, long_memory, short_memory):\n",
    "        # Forget Gate\n",
    "        long_rememeber_percent = torch.sigmoid((short_memory * self.wlr1) + (input_value * self.wlr2) + self.blr1)\n",
    "        forget_long_memory = long_memory * long_rememeber_percent\n",
    "        \n",
    "        # Input Gate\n",
    "        potential_remember_percent = torch.sigmoid((short_memory * self.wpr1) + (input_value * self.wpr2) + self.bpr1)\n",
    "        potential_memory = torch.tanh((short_memory * self.wp1) + (short_memory * self.wp2) + self.bp1)\n",
    "        updated_long_memory = forget_long_memory + (potential_memory * potential_remember_percent)\n",
    "\n",
    "        # Output Gate\n",
    "        output_percent = torch.sigmoid((short_memory * self.wo1) + (input_value * self.wo2) + self.bo1)\n",
    "        updated_short_memory = torch.tanh(updated_long_memory) * output_percent\n",
    "\n",
    "        # Pass LTM and STM to another Unit\n",
    "        return ([updated_long_memory, updated_short_memory])\n",
    "        \n",
    "    # Makes a forward pass through the unrolled LSTM unit\n",
    "    def forward(sefl, input):\n",
    "        # # Input is a sequence data \n",
    "        # long_memory = 0\n",
    "        # short_memory = 0\n",
    "\n",
    "        # for t in range(input.size(0)): # input.size(0) will return seq_length\n",
    "        #     input_t = input[t] # Get data in t time\n",
    "        #     long_memory, short_memory = sefl.lstm_unit(input_t, long_memory, short_memory)\n",
    "\n",
    "        long_memory = 0\n",
    "        short_memory = 0\n",
    "        day1 = input[0]\n",
    "        day2 = input[1]\n",
    "        day3 = input[2]\n",
    "        day4 = input[3]\n",
    "\n",
    "        long_memory, short_memory = sefl.lstm_unit(day1, long_memory, short_memory)\n",
    "        long_memory, short_memory = sefl.lstm_unit(day2, long_memory, short_memory)\n",
    "        long_memory, short_memory = sefl.lstm_unit(day3, long_memory, short_memory)\n",
    "        long_memory, short_memory = sefl.lstm_unit(day4, long_memory, short_memory)\n",
    "\n",
    "        return short_memory\n",
    "\n",
    "    # Configure the optimizer we want to use:\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr= 0.1)\n",
    "        \n",
    "    # Calculate the loss and log trainning progress\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_i, label_i = batch\n",
    "        output_i = self.forward(input_i[0])\n",
    "    \n",
    "        # Tính toán loss\n",
    "        loss = (output_i - label_i) ** 2\n",
    "    \n",
    "        # Log giá trị trung bình của loss (scalar)\n",
    "        self.log(\"train_loss\", loss)\n",
    "    \n",
    "        # Nếu `label_i` có nhiều phần tử, cần duyệt qua từng phần tử\n",
    "        for i, label in enumerate(label_i):\n",
    "            if label.item() == 0:\n",
    "                self.log(f'out_0_batch_{i}', output_i)\n",
    "            else:\n",
    "                self.log(f'out_1_batch_{i}', output_i)\n",
    "    \n",
    "        # Trả về giá trị loss trung bình\n",
    "        return loss        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4fb1e16-ff84-4ad4-9a16-b5f9dc7620a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted =  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "model = LSTMByHand()\n",
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted = \", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f092d35-e5ee-44c9-87e9-cf7657cb322a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company B: Observed = 1, Predicted =  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company B: Observed = 1, Predicted = \", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb00528d-c9f8-4b9a-86e6-46397071e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])\n",
    "labels = torch.tensor([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71a41447-930f-4d94-ab5c-1785f05b0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo Dataset và DataLoader\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)\n",
    "\n",
    "# Tối ưu CPU\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b8008a5-f6ee-4ebf-b3eb-4e662d782bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 12     | n/a \n",
      "---------------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "D:\\anaconda\\src\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "D:\\anaconda\\src\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad12cd4d94b345289bc4caeb9e0fa4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "trainer = l.Trainer(max_epochs=2000)\n",
    "print(\"Starting training...\")\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9fd0bb9-9bb1-4a2b-9e17-8445edf3d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted =  tensor(0.0669)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted = \", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "885331af-964a-4d43-b3a9-df685f06f823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company B: Observed = 1, Predicted =  tensor(0.9721)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company B: Observed = 1, Predicted = \", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83e2e42a-acb1-4c14-a58e-ffb5784cd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e59b467c-73ff-4dc2-aca8-597cdc027057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at D:\\anaconda\\srcCode\\ExampleStockPrediction\\lightning_logs\\version_48\\checkpoints\\epoch=1999-step=4000.ckpt\n",
      "D:\\anaconda\\src\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:362: The dirpath has changed from 'D:\\\\anaconda\\\\srcCode\\\\ExampleStockPrediction\\\\lightning_logs\\\\version_48\\\\checkpoints' to 'D:\\\\anaconda\\\\srcCode\\\\ExampleStockPrediction\\\\lightning_logs\\\\version_49\\\\checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 12     | n/a \n",
      "---------------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at D:\\anaconda\\srcCode\\ExampleStockPrediction\\lightning_logs\\version_48\\checkpoints\\epoch=1999-step=4000.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227fc5e2481c47479b6e92e0b0c7f3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3000` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = l.Trainer(max_epochs=3000)\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fb50c38-acb2-4024-8516-4d91a8b70717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted =  tensor(0.0768)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")  \n",
    "print(\"Company A: Observed = 0, Predicted = \", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b5f19b3-d5b4-4c8d-bc43-5aff3a09abc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company B: Observed = 1, Predicted =  tensor(0.9702)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company B: Observed = 1, Predicted = \", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e95efbb-a08b-43e9-990e-b3ea48880a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at D:\\anaconda\\srcCode\\ExampleStockPrediction\\lightning_logs\\version_49\\checkpoints\\epoch=2999-step=6000.ckpt\n",
      "D:\\anaconda\\src\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:362: The dirpath has changed from 'D:\\\\anaconda\\\\srcCode\\\\ExampleStockPrediction\\\\lightning_logs\\\\version_49\\\\checkpoints' to 'D:\\\\anaconda\\\\srcCode\\\\ExampleStockPrediction\\\\lightning_logs\\\\version_50\\\\checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 12     | n/a \n",
      "---------------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at D:\\anaconda\\srcCode\\ExampleStockPrediction\\lightning_logs\\version_49\\checkpoints\\epoch=2999-step=6000.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce950722aac415b8e0e745608b7afde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5000` reached.\n"
     ]
    }
   ],
   "source": [
    "path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path\n",
    "trainer = l.Trainer(max_epochs=5000)\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bec4df2b-a6ff-436f-a9e1-1d99bb8b9412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted =  tensor(0.0421)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")  \n",
    "print(\"Company A: Observed = 0, Predicted = \", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bddff589-5d8e-4a15-9b56-f2daea42a8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company B: Observed = 1, Predicted =  tensor(0.9703)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company B: Observed = 1, Predicted = \", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18774503-4799-4908-ac57-4a82f9aad164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at D:\\anaconda\\srcCode\\ExampleStockPrediction\\lightning_logs\\version_50\\checkpoints\\epoch=4999-step=10000.ckpt\n",
      "D:\\anaconda\\src\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:362: The dirpath has changed from 'D:\\\\anaconda\\\\srcCode\\\\ExampleStockPrediction\\\\lightning_logs\\\\version_50\\\\checkpoints' to 'D:\\\\anaconda\\\\srcCode\\\\ExampleStockPrediction\\\\lightning_logs\\\\version_51\\\\checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 12     | n/a \n",
      "---------------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at D:\\anaconda\\srcCode\\ExampleStockPrediction\\lightning_logs\\version_50\\checkpoints\\epoch=4999-step=10000.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3508a8525da64b18adf79ad64b338688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path\n",
    "trainer = l.Trainer(max_epochs=10000)\n",
    "print(\"Starting training...\")\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_best_checkpoint)\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae9f8df5-6bb2-426a-afb8-4136cfcb3401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted =  tensor(0.0019)\n",
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company B: Observed = 1, Predicted =  tensor(0.9957)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")  \n",
    "print(\"Company A: Observed = 0, Predicted = \", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company B: Observed = 1, Predicted = \", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9c961c3-41fe-4b57-b6a1-82f972f5639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at D:\\anaconda\\srcCode\\ExampleStockPrediction\\lightning_logs\\version_45\\checkpoints\\epoch=9999-step=20000.ckpt\n",
      "D:\\anaconda\\src\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:362: The dirpath has changed from 'D:\\\\anaconda\\\\srcCode\\\\ExampleStockPrediction\\\\lightning_logs\\\\version_45\\\\checkpoints' to 'D:\\\\anaconda\\\\srcCode\\\\ExampleStockPrediction\\\\lightning_logs\\\\version_46\\\\checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 12     | n/a \n",
      "---------------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "You restored a checkpoint with current_epoch=9999, but you have set Trainer(max_epochs=2000).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m l\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, train_dataloaders\u001b[38;5;241m=\u001b[39mdataloader, ckpt_path\u001b[38;5;241m=\u001b[39mpath_to_best_checkpoint)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\anaconda\\src\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    540\u001b[0m )\n",
      "File \u001b[1;32mD:\\anaconda\\src\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mD:\\anaconda\\src\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\src\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:972\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# restore optimizers, etc.\u001b[39;00m\n\u001b[0;32m    971\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: restoring training state\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 972\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n",
      "File \u001b[1;32mD:\\anaconda\\src\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py:293\u001b[0m, in \u001b[0;36m_CheckpointConnector.restore_training_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_precision_plugin_state()\n\u001b[0;32m    292\u001b[0m \u001b[38;5;66;03m# restore loops and their progress\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_loops()\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;66;03m# restore optimizers and schedulers state\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\src\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py:351\u001b[0m, in \u001b[0;36m_CheckpointConnector.restore_loops\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# crash if max_epochs is lower then the current epoch from the checkpoint\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_epochs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_epochs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcurrent_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_epochs\n\u001b[0;32m    350\u001b[0m ):\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou restored a checkpoint with current_epoch=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcurrent_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    353\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but you have set Trainer(max_epochs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    354\u001b[0m     )\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: You restored a checkpoint with current_epoch=9999, but you have set Trainer(max_epochs=2000)."
     ]
    }
   ],
   "source": [
    "path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path\n",
    "trainer = l.Trainer(max_epochs=2000)\n",
    "print(\"Starting training...\")\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_best_checkpoint)\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d121e3f-3310-4572-83eb-4bd8d09b55ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1420ad0-4be5-4cca-b758-1a1e1f46b97e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa48ba59-ca6e-44c6-a420-561dc5708820",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actual' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m metrics \u001b[38;5;241m=\u001b[39m calculate_matrics(actual, predicted)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'actual' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30d0a9-1a73-477c-985a-eee9e05f0019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
